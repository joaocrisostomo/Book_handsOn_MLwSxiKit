{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 11 - Training Deep Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.max_rows=500\n",
    "pd.options.display.max_columns=500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1 - Glorot, He, LeCun Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense at 0x292b110eec8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation = \"relu\", kernel_initializer = \"he_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense at 0x292b36eb8c8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode = \"fan_avg\", distribution = 'uniform')\n",
    "\n",
    "keras.layers.Dense(10, activation = \"sigmoid\", kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(10, kernel_initializer = \"he_normal\"),\n",
    "    keras.layers.LeakyReLU(alpha=0.2),\n",
    "    keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 - Batch Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = [28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Trainable vars and 2 non-trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\d12669\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\base_layer.py:1348: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`layer.updates` will be removed in a future version. '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3 Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue = 1.0)\n",
    "model.compile(loss='mse', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4 Transfering Learning w/ Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reusing a Keras model\n",
    "Let's split the fashion MNIST training set in two:\n",
    "\n",
    "X_train_A: all images of all items except for sandals and shirts (classes 5 and 6).\n",
    "X_train_B: a much smaller training set of just the first 200 images of sandals or shirts.\n",
    "The validation set and the test set are also split this way, but without restricting the number of images.\n",
    "\n",
    "We will train a model on set A (classification task with 8 classes), and try to reuse it to tackle set B (binary classification). We hope to transfer a little bit of knowledge from task A to task B, since classes in set A (sneakers, ankle boots, coats, t-shirts, etc.) are somewhat similar to classes in set B (sandals and shirts). However, since we are using Dense layers, only patterns that occur at the same location can be reused (in contrast, convolutional layers will transfer much better, since learned patterns can be detected anywhere on the image, as we will see in the CNN chapter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43986, 28, 28)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 28, 28)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1375/1375 [==============================] - 6s 3ms/step - loss: 0.5926 - accuracy: 0.8103 - val_loss: 0.3890 - val_accuracy: 0.8677\n",
      "Epoch 2/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.3523 - accuracy: 0.8786 - val_loss: 0.3290 - val_accuracy: 0.8822\n",
      "Epoch 3/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.3170 - accuracy: 0.8895 - val_loss: 0.3014 - val_accuracy: 0.8989\n",
      "Epoch 4/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2972 - accuracy: 0.8974 - val_loss: 0.2892 - val_accuracy: 0.9016\n",
      "Epoch 5/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2834 - accuracy: 0.9023 - val_loss: 0.2774 - val_accuracy: 0.9071\n",
      "Epoch 6/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2729 - accuracy: 0.9063 - val_loss: 0.2733 - val_accuracy: 0.9071\n",
      "Epoch 7/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2640 - accuracy: 0.9091 - val_loss: 0.2719 - val_accuracy: 0.9088\n",
      "Epoch 8/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2572 - accuracy: 0.9125 - val_loss: 0.2589 - val_accuracy: 0.9141\n",
      "Epoch 9/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2518 - accuracy: 0.9136 - val_loss: 0.2562 - val_accuracy: 0.9143\n",
      "Epoch 10/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2468 - accuracy: 0.9155 - val_loss: 0.2542 - val_accuracy: 0.9155\n",
      "Epoch 11/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2422 - accuracy: 0.9177 - val_loss: 0.2496 - val_accuracy: 0.9155\n",
      "Epoch 12/20\n",
      "1375/1375 [==============================] - 5s 3ms/step - loss: 0.2382 - accuracy: 0.9187 - val_loss: 0.2511 - val_accuracy: 0.9128\n",
      "Epoch 13/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2350 - accuracy: 0.9197 - val_loss: 0.2446 - val_accuracy: 0.9158\n",
      "Epoch 14/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2315 - accuracy: 0.9214 - val_loss: 0.2413 - val_accuracy: 0.9175\n",
      "Epoch 15/20\n",
      "1375/1375 [==============================] - 5s 3ms/step - loss: 0.2287 - accuracy: 0.9214 - val_loss: 0.2447 - val_accuracy: 0.9195\n",
      "Epoch 16/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2253 - accuracy: 0.9226 - val_loss: 0.2386 - val_accuracy: 0.9193\n",
      "Epoch 17/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2230 - accuracy: 0.9232 - val_loss: 0.2403 - val_accuracy: 0.9178\n",
      "Epoch 18/20\n",
      "1375/1375 [==============================] - 10s 7ms/step - loss: 0.2200 - accuracy: 0.9244 - val_loss: 0.2425 - val_accuracy: 0.9155\n",
      "Epoch 19/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2177 - accuracy: 0.9254 - val_loss: 0.2331 - val_accuracy: 0.9203\n",
      "Epoch 20/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2155 - accuracy: 0.9262 - val_loss: 0.2332 - val_accuracy: 0.9205\n"
     ]
    }
   ],
   "source": [
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                    validation_data=(X_valid_A, y_valid_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.save(\"my_model_A.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_on_A = keras.models.Sequential(model_A_clone.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 2s 97ms/step - loss: 0.1826 - accuracy: 0.9600 - val_loss: 0.1744 - val_accuracy: 0.9777\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 0.1434 - accuracy: 0.9750 - val_loss: 0.1459 - val_accuracy: 0.9828\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 41ms/step - loss: 0.1188 - accuracy: 0.9850 - val_loss: 0.1261 - val_accuracy: 0.9848\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 42ms/step - loss: 0.1015 - accuracy: 0.9850 - val_loss: 0.1122 - val_accuracy: 0.9858\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\d12669\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "7/7 [==============================] - 2s 81ms/step - loss: 0.0929 - accuracy: 0.9850 - val_loss: 0.1114 - val_accuracy: 0.9858\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.0921 - accuracy: 0.9850 - val_loss: 0.1106 - val_accuracy: 0.9858\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0912 - accuracy: 0.9850 - val_loss: 0.1098 - val_accuracy: 0.9858\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0904 - accuracy: 0.9850 - val_loss: 0.1090 - val_accuracy: 0.9858\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.0896 - accuracy: 0.9850 - val_loss: 0.1082 - val_accuracy: 0.9858\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.0888 - accuracy: 0.9850 - val_loss: 0.1074 - val_accuracy: 0.9858\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.0880 - accuracy: 0.9850 - val_loss: 0.1067 - val_accuracy: 0.9858\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.0872 - accuracy: 0.9850 - val_loss: 0.1060 - val_accuracy: 0.9858\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0865 - accuracy: 0.9850 - val_loss: 0.1052 - val_accuracy: 0.9858\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0857 - accuracy: 0.9850 - val_loss: 0.1045 - val_accuracy: 0.9858\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0849 - accuracy: 0.9850 - val_loss: 0.1038 - val_accuracy: 0.9858\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0842 - accuracy: 0.9850 - val_loss: 0.1030 - val_accuracy: 0.9858\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0834 - accuracy: 0.9850 - val_loss: 0.1023 - val_accuracy: 0.9858\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.0827 - accuracy: 0.9850 - val_loss: 0.1017 - val_accuracy: 0.9858\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.0820 - accuracy: 0.9850 - val_loss: 0.1010 - val_accuracy: 0.9858\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.0813 - accuracy: 0.9850 - val_loss: 0.1003 - val_accuracy: 0.9858\n"
     ]
    }
   ],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "optimizer = keras.optimizers.SGD(lr=1e-4)\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer,metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16, validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0965 - accuracy: 0.9905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09646978974342346, 0.9904999732971191]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers - Learning Rate with Exponential Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch):\n",
    "    return 0.01 * 0.1**(epoch/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 01.**(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 9s 4ms/step - loss: 27.6198\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "history = model.fit(X_train, y_train, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoiding Overfitting Through Regularization l1, l2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                           activation = \"elu\",\n",
    "                           kernel_initializer = \"he_normal\",\n",
    "                           kernel_regularizer = keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\", kernel_initializer=\"glorot_uniform\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 8:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using CIFAR image Dataset:\n",
    "- A) Build DNN w/ 20 hidden Layers of 100 neurons, HE initialization and ELU Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "model.add(keras.layers.Dense(100, activation= \"elu\", kernel_initializer = \"he_normal\"))\n",
    "model.add(keras.layers.Dense(100, activation= \"elu\", kernel_initializer = \"he_normal\"))\n",
    "model.add(keras.layers.Dense(100, activation= \"elu\", kernel_initializer = \"he_normal\"))\n",
    "model.add(keras.layers.Dense(100, activation= \"elu\", kernel_initializer = \"he_normal\"))\n",
    "model.add(keras.layers.Dense(100, activation= \"elu\", kernel_initializer = \"he_normal\")) #5\n",
    "model.add(keras.layers.Dense(100, activation= \"elu\", kernel_initializer = \"he_normal\"))\n",
    "model.add(keras.layers.Dense(100, activation= \"elu\", kernel_initializer = \"he_normal\"))\n",
    "model.add(keras.layers.Dense(100, activation= \"elu\", kernel_initializer = \"he_normal\"))\n",
    "model.add(keras.layers.Dense(100, activation= \"elu\", kernel_initializer = \"he_normal\"))\n",
    "model.add(keras.layers.Dense(100, activation= \"elu\", kernel_initializer = \"he_normal\")) #10\n",
    "model.add(keras.layers.Dense(100, activation= \"elu\", kernel_initializer = \"he_normal\"))\n",
    "model.add(keras.layers.Dense(100, activation= \"elu\", kernel_initializer = \"he_normal\"))\n",
    "model.add(keras.layers.Dense(100, activation= \"elu\", kernel_initializer = \"he_normal\"))\n",
    "model.add(keras.layers.Dense(100, activation= \"elu\", kernel_initializer = \"he_normal\"))\n",
    "model.add(keras.layers.Dense(100, activation= \"elu\", kernel_initializer = \"he_normal\")) #15\n",
    "model.add(keras.layers.Dense(100, activation= \"elu\", kernel_initializer = \"he_normal\"))\n",
    "model.add(keras.layers.Dense(100, activation= \"elu\", kernel_initializer = \"he_normal\"))\n",
    "model.add(keras.layers.Dense(100, activation= \"elu\", kernel_initializer = \"he_normal\"))\n",
    "model.add(keras.layers.Dense(100, activation= \"elu\", kernel_initializer = \"he_normal\"))\n",
    "model.add(keras.layers.Dense(100, activation= \"elu\", kernel_initializer = \"he_normal\")) #20\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- b) Optimizer = Nadam and Early Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-5)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 23s 13ms/step - loss: 3.2163 - accuracy: 0.1716 - val_loss: 2.1691 - val_accuracy: 0.2166\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 2.0405 - accuracy: 0.2516 - val_loss: 2.0431 - val_accuracy: 0.2554\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.9308 - accuracy: 0.2963 - val_loss: 1.9215 - val_accuracy: 0.2956\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.8603 - accuracy: 0.3268 - val_loss: 1.8517 - val_accuracy: 0.3358\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.8012 - accuracy: 0.3511 - val_loss: 1.7883 - val_accuracy: 0.3534\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.7525 - accuracy: 0.3672 - val_loss: 1.7379 - val_accuracy: 0.3788\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.7096 - accuracy: 0.3813 - val_loss: 1.7402 - val_accuracy: 0.3768\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.6785 - accuracy: 0.3969 - val_loss: 1.6908 - val_accuracy: 0.3974\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.6493 - accuracy: 0.4057 - val_loss: 1.6629 - val_accuracy: 0.3988\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.6261 - accuracy: 0.4150 - val_loss: 1.6709 - val_accuracy: 0.3966\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.6022 - accuracy: 0.4235 - val_loss: 1.6530 - val_accuracy: 0.4018\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.5856 - accuracy: 0.4308 - val_loss: 1.6739 - val_accuracy: 0.4022\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.5683 - accuracy: 0.4370 - val_loss: 1.6350 - val_accuracy: 0.4106\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.5513 - accuracy: 0.4422 - val_loss: 1.6007 - val_accuracy: 0.4236\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.5344 - accuracy: 0.4480 - val_loss: 1.6331 - val_accuracy: 0.4204\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.5212 - accuracy: 0.4543 - val_loss: 1.5975 - val_accuracy: 0.4282\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.5046 - accuracy: 0.4589 - val_loss: 1.5847 - val_accuracy: 0.4308\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.4949 - accuracy: 0.4649 - val_loss: 1.5696 - val_accuracy: 0.4382\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.4796 - accuracy: 0.4698 - val_loss: 1.5789 - val_accuracy: 0.4336\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.4710 - accuracy: 0.4712 - val_loss: 1.5781 - val_accuracy: 0.4308\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.4580 - accuracy: 0.4759 - val_loss: 1.5669 - val_accuracy: 0.4380\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.4493 - accuracy: 0.4767 - val_loss: 1.5754 - val_accuracy: 0.4396\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.4393 - accuracy: 0.4801 - val_loss: 1.5943 - val_accuracy: 0.4380\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.4283 - accuracy: 0.4869 - val_loss: 1.5712 - val_accuracy: 0.4430\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.4213 - accuracy: 0.4896 - val_loss: 1.5564 - val_accuracy: 0.4534\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.4114 - accuracy: 0.4916 - val_loss: 1.5699 - val_accuracy: 0.4410\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3996 - accuracy: 0.4984 - val_loss: 1.5434 - val_accuracy: 0.4566\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.3931 - accuracy: 0.5016 - val_loss: 1.5588 - val_accuracy: 0.4474\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 18s 12ms/step - loss: 1.3843 - accuracy: 0.5040 - val_loss: 1.6088 - val_accuracy: 0.4410\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3750 - accuracy: 0.5068 - val_loss: 1.5538 - val_accuracy: 0.4604\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3670 - accuracy: 0.5061 - val_loss: 1.5910 - val_accuracy: 0.4476\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3606 - accuracy: 0.5103 - val_loss: 1.5404 - val_accuracy: 0.4620\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.3544 - accuracy: 0.5120 - val_loss: 1.5353 - val_accuracy: 0.4548\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.3430 - accuracy: 0.5169 - val_loss: 1.5581 - val_accuracy: 0.4562\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.3329 - accuracy: 0.5214 - val_loss: 1.5436 - val_accuracy: 0.4606\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.3292 - accuracy: 0.5238 - val_loss: 1.5454 - val_accuracy: 0.4642\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.3220 - accuracy: 0.5257 - val_loss: 1.5462 - val_accuracy: 0.4592\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.3133 - accuracy: 0.5244 - val_loss: 1.5470 - val_accuracy: 0.4604\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.3033 - accuracy: 0.5331 - val_loss: 1.5614 - val_accuracy: 0.4518\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2974 - accuracy: 0.5342 - val_loss: 1.5446 - val_accuracy: 0.4592\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.2923 - accuracy: 0.5346 - val_loss: 1.5472 - val_accuracy: 0.4662\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.2849 - accuracy: 0.5361 - val_loss: 1.6102 - val_accuracy: 0.4466\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2778 - accuracy: 0.5412 - val_loss: 1.5725 - val_accuracy: 0.4614\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2670 - accuracy: 0.5457 - val_loss: 1.5858 - val_accuracy: 0.4638\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.2596 - accuracy: 0.5480 - val_loss: 1.5646 - val_accuracy: 0.4604\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.2558 - accuracy: 0.5500 - val_loss: 1.5734 - val_accuracy: 0.4622\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.2453 - accuracy: 0.5533 - val_loss: 1.5817 - val_accuracy: 0.4624\n",
      "Epoch 48/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.2376 - accuracy: 0.5556 - val_loss: 1.6061 - val_accuracy: 0.4590\n",
      "Epoch 49/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.2319 - accuracy: 0.5558 - val_loss: 1.6352 - val_accuracy: 0.4514\n",
      "Epoch 50/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.2259 - accuracy: 0.5597 - val_loss: 1.5859 - val_accuracy: 0.4512\n",
      "Epoch 51/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.2203 - accuracy: 0.5612 - val_loss: 1.6145 - val_accuracy: 0.4562\n",
      "Epoch 52/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2128 - accuracy: 0.5635 - val_loss: 1.6110 - val_accuracy: 0.4558\n",
      "Epoch 53/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.2053 - accuracy: 0.5670 - val_loss: 1.5834 - val_accuracy: 0.4628\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs = 100,\n",
    "                    validation_data = (X_valid, y_valid),\n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 3ms/step - loss: 1.5353 - accuracy: 0.4548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.535338282585144, 0.454800009727478]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_cifar10_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- c) Add Batch Mormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.models.Sequential()\n",
    "model2.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "model2.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model2.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    model2.add(keras.layers.BatchNormalization())\n",
    "    model2.add(keras.layers.Activation(\"elu\"))\n",
    "model2.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model2.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_model2.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run2_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 58s 30ms/step - loss: 1.8334 - accuracy: 0.3436 - val_loss: 1.6120 - val_accuracy: 0.4194\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.6600 - accuracy: 0.4087 - val_loss: 1.5699 - val_accuracy: 0.4344\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.5908 - accuracy: 0.4340 - val_loss: 1.5271 - val_accuracy: 0.4522\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.5387 - accuracy: 0.4540 - val_loss: 1.4814 - val_accuracy: 0.4736\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.4962 - accuracy: 0.4703 - val_loss: 1.4433 - val_accuracy: 0.4864\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.4573 - accuracy: 0.4841 - val_loss: 1.4308 - val_accuracy: 0.4864\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.4288 - accuracy: 0.4936 - val_loss: 1.3837 - val_accuracy: 0.5110\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.4023 - accuracy: 0.5036 - val_loss: 1.3919 - val_accuracy: 0.5026\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.3722 - accuracy: 0.5158 - val_loss: 1.3773 - val_accuracy: 0.5112\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.3514 - accuracy: 0.5230 - val_loss: 1.3881 - val_accuracy: 0.5074\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.3295 - accuracy: 0.5291 - val_loss: 1.3356 - val_accuracy: 0.5252\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.3198 - accuracy: 0.5325 - val_loss: 1.3570 - val_accuracy: 0.5254\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.2938 - accuracy: 0.5444 - val_loss: 1.3506 - val_accuracy: 0.5216\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.2745 - accuracy: 0.5504 - val_loss: 1.3640 - val_accuracy: 0.5184os\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.2540 - accuracy: 0.5570 - val_loss: 1.3652 - val_accuracy: 0.5218\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.2365 - accuracy: 0.5641 - val_loss: 1.3475 - val_accuracy: 0.5234\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.2262 - accuracy: 0.5686 - val_loss: 1.3281 - val_accuracy: 0.5318\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.2146 - accuracy: 0.5727 - val_loss: 1.3256 - val_accuracy: 0.5356\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.1975 - accuracy: 0.5781 - val_loss: 1.3479 - val_accuracy: 0.5304\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.1869 - accuracy: 0.5833 - val_loss: 1.3627 - val_accuracy: 0.5256\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.1670 - accuracy: 0.5886 - val_loss: 1.3188 - val_accuracy: 0.5318\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.1577 - accuracy: 0.5905 - val_loss: 1.3233 - val_accuracy: 0.5364\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.1436 - accuracy: 0.5964 - val_loss: 1.3581 - val_accuracy: 0.5274\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.1315 - accuracy: 0.6017 - val_loss: 1.3216 - val_accuracy: 0.5424\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.1257 - accuracy: 0.6043 - val_loss: 1.3465 - val_accuracy: 0.5316\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.1114 - accuracy: 0.6074 - val_loss: 1.3405 - val_accuracy: 0.5328\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.0931 - accuracy: 0.6149 - val_loss: 1.3432 - val_accuracy: 0.5384\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.0840 - accuracy: 0.6183 - val_loss: 1.3662 - val_accuracy: 0.5216\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.0778 - accuracy: 0.6190 - val_loss: 1.3310 - val_accuracy: 0.5324\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.0632 - accuracy: 0.6263 - val_loss: 1.3384 - val_accuracy: 0.5398\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.0564 - accuracy: 0.6286 - val_loss: 1.3454 - val_accuracy: 0.5374\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.0488 - accuracy: 0.6318 - val_loss: 1.3512 - val_accuracy: 0.5382\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.0344 - accuracy: 0.6386 - val_loss: 1.3548 - val_accuracy: 0.5356\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.0221 - accuracy: 0.6393 - val_loss: 1.3781 - val_accuracy: 0.5364\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.0202 - accuracy: 0.6411 - val_loss: 1.3445 - val_accuracy: 0.5426\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.0107 - accuracy: 0.6474 - val_loss: 1.3568 - val_accuracy: 0.5416\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 0.9967 - accuracy: 0.6487 - val_loss: 1.3790 - val_accuracy: 0.5318\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 0.9881 - accuracy: 0.6525 - val_loss: 1.3394 - val_accuracy: 0.5458\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 0.9840 - accuracy: 0.6527 - val_loss: 1.3636 - val_accuracy: 0.5408\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 0.9749 - accuracy: 0.6588 - val_loss: 1.3889 - val_accuracy: 0.5294\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 0.9621 - accuracy: 0.6614 - val_loss: 1.3700 - val_accuracy: 0.5378\n"
     ]
    }
   ],
   "source": [
    "history = model2.fit(X_train, y_train, epochs = 100,\n",
    "                    validation_data = (X_valid, y_valid),\n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 4ms/step - loss: 1.3188 - accuracy: 0.5318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3188129663467407, 0.5317999720573425]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_cifar10_model2.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- d) Replace BN with SELU and Lecun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = keras.models.Sequential()\n",
    "model3.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model3.add(keras.layers.Dense(100, activation=\"selu\",\n",
    "                                 kernel_initializer=\"lecun_normal\"))\n",
    "model3.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model3.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_model3.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run3_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_means = X_train_full.mean(axis=0)\n",
    "X_stds = X_train_full.std(axis=0)\n",
    "X_train_scaled = (X_train_full - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 32s 17ms/step - loss: 2.0522 - accuracy: 0.2374 - val_loss: 1.9350 - val_accuracy: 0.2614\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.8845 - accuracy: 0.2976 - val_loss: 1.8900 - val_accuracy: 0.2962\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.8170 - accuracy: 0.3373 - val_loss: 1.7792 - val_accuracy: 0.3404\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.7633 - accuracy: 0.3575 - val_loss: 1.7131 - val_accuracy: 0.3786\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.7221 - accuracy: 0.3761 - val_loss: 1.7337 - val_accuracy: 0.3624\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6910 - accuracy: 0.3887 - val_loss: 1.6809 - val_accuracy: 0.3820\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6638 - accuracy: 0.3998 - val_loss: 1.7375 - val_accuracy: 0.3792\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6425 - accuracy: 0.4091 - val_loss: 1.5952 - val_accuracy: 0.4128\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 1.6235 - accuracy: 0.4154 - val_loss: 1.5973 - val_accuracy: 0.4268\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.6070 - accuracy: 0.4227 - val_loss: 1.6135 - val_accuracy: 0.4164\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5961 - accuracy: 0.4256 - val_loss: 1.6060 - val_accuracy: 0.4148\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.5840 - accuracy: 0.4324 - val_loss: 1.5133 - val_accuracy: 0.4612\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5685 - accuracy: 0.4402 - val_loss: 1.5191 - val_accuracy: 0.4558\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5594 - accuracy: 0.4443 - val_loss: 1.5112 - val_accuracy: 0.4604\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5443 - accuracy: 0.4492 - val_loss: 1.5224 - val_accuracy: 0.4504\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 1.5379 - accuracy: 0.4510 - val_loss: 1.5326 - val_accuracy: 0.4492\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5240 - accuracy: 0.4592 - val_loss: 1.4853 - val_accuracy: 0.4718\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5139 - accuracy: 0.4635 - val_loss: 1.5105 - val_accuracy: 0.4652\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.5076 - accuracy: 0.4651 - val_loss: 1.4656 - val_accuracy: 0.4714\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.5011 - accuracy: 0.4667 - val_loss: 1.4969 - val_accuracy: 0.4678\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4883 - accuracy: 0.4708 - val_loss: 1.4142 - val_accuracy: 0.5032\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4792 - accuracy: 0.4717 - val_loss: 1.4311 - val_accuracy: 0.4882\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.4713 - accuracy: 0.4783 - val_loss: 1.4339 - val_accuracy: 0.4872\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.4670 - accuracy: 0.4809 - val_loss: 1.4487 - val_accuracy: 0.4918\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4590 - accuracy: 0.4842 - val_loss: 1.4209 - val_accuracy: 0.4950\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4560 - accuracy: 0.4824 - val_loss: 1.4303 - val_accuracy: 0.4904\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4464 - accuracy: 0.4855 - val_loss: 1.4994 - val_accuracy: 0.4592\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4460 - accuracy: 0.4857 - val_loss: 1.4407 - val_accuracy: 0.4918\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4347 - accuracy: 0.4915 - val_loss: 1.4016 - val_accuracy: 0.5058\n",
      "Epoch 30/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4309 - accuracy: 0.4912 - val_loss: 1.4057 - val_accuracy: 0.5032\n",
      "Epoch 31/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4241 - accuracy: 0.4952 - val_loss: 1.4198 - val_accuracy: 0.4980\n",
      "Epoch 32/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4183 - accuracy: 0.4981 - val_loss: 1.4200 - val_accuracy: 0.4944\n",
      "Epoch 33/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4129 - accuracy: 0.4985 - val_loss: 1.3825 - val_accuracy: 0.5066\n",
      "Epoch 34/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4094 - accuracy: 0.4982 - val_loss: 1.4353 - val_accuracy: 0.4882\n",
      "Epoch 35/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4008 - accuracy: 0.5017 - val_loss: 1.4124 - val_accuracy: 0.4972\n",
      "Epoch 36/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.3950 - accuracy: 0.5048 - val_loss: 1.3315 - val_accuracy: 0.5244\n",
      "Epoch 37/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.3880 - accuracy: 0.5082 - val_loss: 1.3447 - val_accuracy: 0.5226\n",
      "Epoch 38/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.3856 - accuracy: 0.5108 - val_loss: 1.3323 - val_accuracy: 0.5216\n",
      "Epoch 39/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.3793 - accuracy: 0.5092 - val_loss: 1.3454 - val_accuracy: 0.5338\n",
      "Epoch 40/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.3712 - accuracy: 0.5128 - val_loss: 1.3570 - val_accuracy: 0.5262\n",
      "Epoch 41/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.3627 - accuracy: 0.5163 - val_loss: 1.3712 - val_accuracy: 0.5214\n",
      "Epoch 42/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.3698 - accuracy: 0.5168 - val_loss: 1.3547 - val_accuracy: 0.5266\n",
      "Epoch 43/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 144.0459 - accuracy: 0.3539 - val_loss: 1.7697 - val_accuracy: 0.3392\n",
      "Epoch 44/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.7613 - accuracy: 0.3471 - val_loss: 1.7222 - val_accuracy: 0.3648\n",
      "Epoch 45/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.7283 - accuracy: 0.3655 - val_loss: 1.7105 - val_accuracy: 0.3762\n",
      "Epoch 46/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.7036 - accuracy: 0.3746 - val_loss: 1.6795 - val_accuracy: 0.3868\n",
      "Epoch 47/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6833 - accuracy: 0.3839 - val_loss: 1.6575 - val_accuracy: 0.3968\n",
      "Epoch 48/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.6664 - accuracy: 0.3932 - val_loss: 1.6391 - val_accuracy: 0.3998\n",
      "Epoch 49/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6508 - accuracy: 0.3970 - val_loss: 1.6407 - val_accuracy: 0.3990\n",
      "Epoch 50/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6385 - accuracy: 0.4021 - val_loss: 1.6026 - val_accuracy: 0.4132\n",
      "Epoch 51/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6373 - accuracy: 0.4035 - val_loss: 1.5915 - val_accuracy: 0.4172\n",
      "Epoch 52/100\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.6186 - accuracy: 0.4115 - val_loss: 1.5914 - val_accuracy: 0.4202\n",
      "Epoch 53/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6104 - accuracy: 0.4148 - val_loss: 1.5820 - val_accuracy: 0.4216\n",
      "Epoch 54/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 1.6030 - accuracy: 0.4188 - val_loss: 1.5765 - val_accuracy: 0.4256\n",
      "Epoch 55/100\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 1.5893 - accuracy: 0.4243 - val_loss: 1.6050 - val_accuracy: 0.4184\n",
      "Epoch 56/100\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5823 - accuracy: 0.4288 - val_loss: 1.5541 - val_accuracy: 0.4360\n"
     ]
    }
   ],
   "source": [
    "history = model3.fit(X_train_full, y_train_full, epochs = 100,\n",
    "                    validation_data = (X_valid, y_valid),\n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 3ms/step - loss: 1.3315 - accuracy: 0.5244\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3315497636795044, 0.524399995803833]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = keras.models.load_model(\"my_cifar10_model3.h5\")\n",
    "model3.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- e) Add Alpha Dropout regularization. Then without retraining try to achieve better performance with MC Dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = keras.models.Sequential()\n",
    "model4.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model4.add(keras.layers.Dense(100, activation=\"selu\",\n",
    "                                 kernel_initializer=\"lecun_normal\"))\n",
    "    model4.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model4.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model4.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_model4.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run4_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_means = X_train_full.mean(axis=0)\n",
    "X_stds = X_train_full.std(axis=0)\n",
    "X_train_scaled = (X_train_full - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 36s 19ms/step - loss: 2.2234 - accuracy: 0.1584 - val_loss: 7.8678 - val_accuracy: 0.1344\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 2.0690 - accuracy: 0.1860 - val_loss: 6.0078 - val_accuracy: 0.1392\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 2.0440 - accuracy: 0.1933 - val_loss: 9.1015 - val_accuracy: 0.1460\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 2.0628 - accuracy: 0.1881 - val_loss: 6.5546 - val_accuracy: 0.1450\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 2.0375 - accuracy: 0.1946 - val_loss: 6.4283 - val_accuracy: 0.1614\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 2.0429 - accuracy: 0.1899 - val_loss: 3.3806 - val_accuracy: 0.1744\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 2.0336 - accuracy: 0.2005 - val_loss: 6.6519 - val_accuracy: 0.1652\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 2.0150 - accuracy: 0.2095 - val_loss: 6.4756 - val_accuracy: 0.1366\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.9992 - accuracy: 0.2140 - val_loss: 3.6135 - val_accuracy: 0.1772\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 1.9957 - accuracy: 0.2155 - val_loss: 4.9820 - val_accuracy: 0.1566\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.9963 - accuracy: 0.2237 - val_loss: 3.9608 - val_accuracy: 0.1692\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 2.0178 - accuracy: 0.2190 - val_loss: 3.7030 - val_accuracy: 0.1562\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 2.0033 - accuracy: 0.2253 - val_loss: 3.1404 - val_accuracy: 0.1390\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.9913 - accuracy: 0.2264 - val_loss: 4.2631 - val_accuracy: 0.1966\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.9925 - accuracy: 0.2297 - val_loss: 3.4451 - val_accuracy: 0.1846\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 2.0114 - accuracy: 0.2148 - val_loss: 2.8003 - val_accuracy: 0.1780\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.9936 - accuracy: 0.2286 - val_loss: 3.8914 - val_accuracy: 0.1862\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.9907 - accuracy: 0.2291 - val_loss: 2.8656 - val_accuracy: 0.1884\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 1.9959 - accuracy: 0.2267 - val_loss: 3.3637 - val_accuracy: 0.1098\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 2.0231 - accuracy: 0.2174 - val_loss: 2.8164 - val_accuracy: 0.1796\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 21s 14ms/step - loss: 2.0144 - accuracy: 0.2204 - val_loss: 3.2988 - val_accuracy: 0.1520\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.9809 - accuracy: 0.2279 - val_loss: 2.9389 - val_accuracy: 0.1868\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.9967 - accuracy: 0.2259 - val_loss: 2.6968 - val_accuracy: 0.1850\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.9959 - accuracy: 0.2284 - val_loss: 3.4714 - val_accuracy: 0.1746\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 2.0101 - accuracy: 0.2201 - val_loss: 4.3873 - val_accuracy: 0.1646\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.9775 - accuracy: 0.2323 - val_loss: 2.9796 - val_accuracy: 0.1636\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 2.0176 - accuracy: 0.2171 - val_loss: 2.9896 - val_accuracy: 0.1452\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 1.9657 - accuracy: 0.2362 - val_loss: 2.9410 - val_accuracy: 0.1602\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.9684 - accuracy: 0.2324 - val_loss: 3.4441 - val_accuracy: 0.1914\n",
      "Epoch 30/100\n",
      "1563/1563 [==============================] - 21s 14ms/step - loss: 1.9933 - accuracy: 0.2225 - val_loss: 3.0339 - val_accuracy: 0.1878\n",
      "Epoch 31/100\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.9747 - accuracy: 0.2341 - val_loss: 3.2694 - val_accuracy: 0.1668\n",
      "Epoch 32/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.9837 - accuracy: 0.2373 - val_loss: 2.8808 - val_accuracy: 0.2122\n",
      "Epoch 33/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 2.0063 - accuracy: 0.2226 - val_loss: 2.8349 - val_accuracy: 0.1478\n",
      "Epoch 34/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 1.9781 - accuracy: 0.2284 - val_loss: 6.4653 - val_accuracy: 0.1756\n",
      "Epoch 35/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.9730 - accuracy: 0.2401 - val_loss: 2.7596 - val_accuracy: 0.1726\n",
      "Epoch 36/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.9554 - accuracy: 0.2372 - val_loss: 3.0004 - val_accuracy: 0.2046\n",
      "Epoch 37/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 1.9623 - accuracy: 0.2470 - val_loss: 2.8627 - val_accuracy: 0.1846\n",
      "Epoch 38/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.9585 - accuracy: 0.2376 - val_loss: 2.9814 - val_accuracy: 0.2114\n",
      "Epoch 39/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.9789 - accuracy: 0.2312 - val_loss: 2.6470 - val_accuracy: 0.1982\n",
      "Epoch 40/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 2.0020 - accuracy: 0.2295 - val_loss: 2.7146 - val_accuracy: 0.1336\n",
      "Epoch 41/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 2.0112 - accuracy: 0.2201 - val_loss: 2.6978 - val_accuracy: 0.1442\n",
      "Epoch 42/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 2.0192 - accuracy: 0.2093 - val_loss: 2.4922 - val_accuracy: 0.1334\n",
      "Epoch 43/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 2.0010 - accuracy: 0.2101 - val_loss: 3.4945 - val_accuracy: 0.1838\n",
      "Epoch 44/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.9979 - accuracy: 0.2131 - val_loss: 2.9720 - val_accuracy: 0.1706\n",
      "Epoch 45/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 1.9872 - accuracy: 0.2145 - val_loss: 4.6642 - val_accuracy: 0.1622\n",
      "Epoch 46/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 2.0560 - accuracy: 0.2141 - val_loss: 3.7207 - val_accuracy: 0.1922\n",
      "Epoch 47/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 2.0230 - accuracy: 0.2165 - val_loss: 5.1895 - val_accuracy: 0.1620\n",
      "Epoch 48/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 2.0908 - accuracy: 0.2113 - val_loss: 3.8594 - val_accuracy: 0.1652\n",
      "Epoch 49/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 2.0037 - accuracy: 0.2244 - val_loss: 3.3777 - val_accuracy: 0.1728\n",
      "Epoch 50/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.9623 - accuracy: 0.2347 - val_loss: 3.5357 - val_accuracy: 0.1804\n",
      "Epoch 51/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 1.9502 - accuracy: 0.2391 - val_loss: 2.6957 - val_accuracy: 0.1884\n",
      "Epoch 52/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.9687 - accuracy: 0.2389 - val_loss: 3.0886 - val_accuracy: 0.1698\n",
      "Epoch 53/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.9665 - accuracy: 0.2387 - val_loss: 2.8320 - val_accuracy: 0.1830\n",
      "Epoch 54/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 1.9511 - accuracy: 0.2381 - val_loss: 3.0033 - val_accuracy: 0.1758\n",
      "Epoch 55/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.9652 - accuracy: 0.2402 - val_loss: 2.9406 - val_accuracy: 0.1956\n",
      "Epoch 56/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.9636 - accuracy: 0.2322 - val_loss: 3.1444 - val_accuracy: 0.1412\n",
      "Epoch 57/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 1.9598 - accuracy: 0.2367 - val_loss: 5.5101 - val_accuracy: 0.1558\n",
      "Epoch 58/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 2.0302 - accuracy: 0.2126 - val_loss: 3.1262 - val_accuracy: 0.0628\n",
      "Epoch 59/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 2.0900 - accuracy: 0.1902 - val_loss: 2.6806 - val_accuracy: 0.1512\n",
      "Epoch 60/100\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 2.3743 - accuracy: 0.2083 - val_loss: 2.7523 - val_accuracy: 0.2064\n",
      "Epoch 61/100\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 2.0029 - accuracy: 0.2111 - val_loss: 3.2193 - val_accuracy: 0.1674\n",
      "Epoch 62/100\n",
      "1563/1563 [==============================] - 21s 14ms/step - loss: 1.9806 - accuracy: 0.2224 - val_loss: 2.9153 - val_accuracy: 0.1582\n"
     ]
    }
   ],
   "source": [
    "history = model4.fit(X_train_full, y_train_full, epochs = 100,\n",
    "                    validation_data = (X_valid, y_valid),\n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 4ms/step - loss: 2.4922 - accuracy: 0.1334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.492168426513672, 0.13339999318122864]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4 = keras.models.load_model(\"my_cifar10_model4.h5\")\n",
    "model4.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Replace Alphadropout to MCDropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model4.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- run model to predict X times\n",
    "- average the predictions of the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict_probas(mc_model, X, n_samples=10):\n",
    "    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
    "    return np.mean(Y_probas, axis=0)\n",
    "\n",
    "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
    "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
    "    return np.argmax(Y_probas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_probas = [mc_model.predict(X_valid) for sample in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_proba = mc_dropout_predict_probas(mc_model, X_valid, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mc_dropout_predict_classes(mc_model, X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1998"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = np.mean(y_pred == y_valid[:, 0])\n",
    "accuracy    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1Cycle Shceduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- f) Retrain the model using 1cycle scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.learning_rate))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.learning_rate)\n",
    "    K.set_value(model.optimizer.learning_rate, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.learning_rate, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** In the on_batch_end() method, logs[\"loss\"] used to contain the batch loss, but in TensorFlow 2.2.0 it was replaced with the mean loss (since the start of the epoch). This explains why the graph below is much smoother than in the book (if you are using TF 2.2 or above). It also means that there is a lag between the moment the batch loss starts exploding and the moment the explosion becomes clear in the graph. So you should choose a slightly smaller learning rate than you would have chosen with the \"noisy\" graph. Alternatively, you can tweak the ExponentialLearningRate callback above so it computes the batch loss (based on the current mean loss and the previous mean loss):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "learning_rate = 0.01\n",
    "decay = 1e-4\n",
    "batch_size = 32\n",
    "n_steps_per_epoch = math.ceil(len(X_train) / batch_size)\n",
    "epochs = np.arange(n_epochs)\n",
    "lrs = learning_rate / (1 + decay * epochs * n_steps_per_epo\n",
    "plt.plot(epochs, lrs,  \"o-\")\n",
    "plt.axis([0, n_epochs - 1, 0, 0.01])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Power Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 50000\n  y sizes: 45000\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16156/3789395728.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_learning_rate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplot_lr_vs_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m1.4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16156/437629292.py\u001b[0m in \u001b[0;36mfind_learning_rate\u001b[1;34m(model, X, y, epochs, batch_size, min_rate, max_rate)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mexp_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExponentialLearningRate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfactor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n\u001b[1;32m---> 21\u001b[1;33m                         callbacks=[exp_lr])\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_lr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1146\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1148\u001b[1;33m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[0;32m   1149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1381\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_cluster_coordinator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1382\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1383\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1150\u001b[1;33m         model=model)\n\u001b[0m\u001b[0;32m   1151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m     \u001b[0m_check_data_cardinality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;31m# If batch_size is not passed but steps is, calculate from the input data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1647\u001b[0m           label, \", \".join(str(i.shape[0]) for i in tf.nest.flatten(single_data)))\n\u001b[0;32m   1648\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1649\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 50000\n  y sizes: 45000\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)\n",
    "plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-2)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OneCycleScheduler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16156/1825270755.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0monecycle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneCycleScheduler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n\u001b[0;32m      4\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                     callbacks=[onecycle])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'OneCycleScheduler' is not defined"
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "onecycle = OneCycleScheduler(math.ceil(len(X_train_scaled) / batch_size) * n_epochs, max_rate=0.05)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (localEnv2)",
   "language": "python",
   "name": "localenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
